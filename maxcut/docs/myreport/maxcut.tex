\documentclass{tufte-handout}
\usepackage{amsmath,amsthm}

\usepackage{pgfplots}
\pgfplotsset{width=\textwidth,compat=1.5.1}

% Program listings.
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset {
				language=Matlab,					% The language of the code.
				basicstyle=\footnotesize,				% The size of the fonts that are used for the code.
				numbers=left,						% Where to put the line-numbers.
				numberstyle=\tiny\color{gray},				% The style that is used for the line-numbers.
				stepnumber=1,						% The step between two line-numbers. If it's 1, each line will be numbered.
				numbersep=5pt,						% How far the line-numbers are from the code.
				backgroundcolor=\color{white},				% Choose the background color. You must add \usepackage{color}.
				showspaces=false,					% Show spaces adding particular underscores.
				showstringspaces=false,				% Underline spaces within strings.
				showtabs=false,					% Show tabs within strings adding particular underscores.
				frame=single,						% Adds a frame around the code.
				rulecolor=\color{black},				% If not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here)).
				tabsize=2,						% Sets default tabsize to 2 spaces.
				captionpos=b,						% Sets the caption-position to bottom.
				breaklines=true,					% Sets automatic line breaking.
				breakatwhitespace=false,				% Sets if automatic breaks should only happen at whitespace.
				title=\lstname,					% Show the filename of files included with \lstinputlisting;  also try caption instead of title.
				keywordstyle=\color{blue},				% Keyword style.
				commentstyle=\color{dkgreen},				% Comment style.
				stringstyle=\color{mauve},				% String literal style.
				escapeinside={\%}{)}					% If you want to add a comment within your code.
				%morekeywords={*,...}					% If you want to add more keywords to the set.
}

\newtheorem{claim}{Claim}[section]
\title{\sf Approximation Algorithm for Maximum Cut}
%\date{\GITAuthorDate}
%\author{Thore Husfeldt}

\begin{document}
\maketitle

\section{Maxcut Lab Report}
by Erik Westrup and Dmitry Basavin.

\subsection{Running time}

The running time of algorithm~R is $\Theta(n + m)$.

\subsection{Randomness}

Algorithm R uses $n$ random bits.

\subsection{Solution quality}

\paragraph{Experiments.}

\begin{enumerate}
\item
For the input file  pw09\_100.9.txt with $t=100$ runs, we found
an average cutsize of $C=12366.41$, roughly $90.54$\% of the optimum
$\operatorname{OPT} = 13658$.
The distribution of cutsizes looks as follows:

\medskip
\noindent
\input{data/R_pw09_100.9.txt.tex}

\item
For the input file matching\_1000.txt with $t=100$ runs, we found
an average cutsize of $C=249.77$, roughly $49.95$\% of the optimum
$\operatorname{OPT} = 500$.
The distribution of cutsizes looks as follows:

\medskip
\noindent
\input{data/R_matching_1000.txt.tex}


\end{enumerate}
\paragraph{Analysis of performance guarantee}

Clearly, Algorithm R performs quite badly on input 
  matching\_1000.txt.
We will show that it can perform \emph{no worse} than that, i.e., we
will establish that in expectation, the cutsize $C$ satisfies $C \geq
\frac{1}{2}\cdot \operatorname{OPT}$.\


We will view $C$ as a random variable that gives the size of the cut
defined by the random choices.
Let $W$ denote the total weight of the edges of $G$, i.e.,
\[ W= \sum_{e\in E} w(e)\,.\]

Then,
\begin{equation}\label{eq: E[C]}
E[C] = \textstyle\frac{1}{2}\cdot W\,.
\end{equation}

To see this, define the indicator random variable $X_{uv}$ for every
edge $uv\in E$ as follows.
Set $X_{uv}=1$ if $uv$ crosses the cut, i.e., $u\in A$ and $v\notin A$
or $u\notin A$ and $v\in A$.
Otherwise, $X_{uv} = 0$.

Then, $\Pr(X_{uv} = 1) = \Pr([u\in A \land v \in V-A] \lor  [v\in A \land u \in V-A])=\frac{1}{4}+\frac{1}{4}=\frac{1}{2}$. This equation holds because the probabilistic events in the square brackets are independent. Now
\begin{equation*}
	E[C]=E \Big [\sum_{(uv)\in E}X_{uv}\cdot w(uv) \Big]
\end{equation*}
Then by the linearity of expectation,
\begin{equation*}
	E[C] = \sum_{(uv)\in E}E[X_{uv}]\cdot w(uv) =
\end{equation*}
\begin{equation*}
	=\sum_{(uv) \in E} Pr(X_{uv}=1)\cdot 1 \cdot w(uv) =
\end{equation*}
\begin{equation*}
	=\sum_{(uv) \in E} \frac{1}{2}\cdot w(uv) =
\end{equation*}
\begin{equation*}
	=\frac{1}{2}\sum_{(uv) \in E} w(uv) =\frac{1}{2} \cdot W
\end{equation*}
Finally, we have 
\(E[C]\geq \frac1{2}\cdot \text{OPT}\) because since all the weights are positive:
\begin{equation*}
W \geq \text{OPT}
\end{equation*}
Hence,
\begin{equation*}
E[C] = \frac1{2} \cdot W \geq \frac1{2}\cdot \text{OPT}
\end{equation*}


\newpage
\section*{Appendix}
\appendix
\lstinputlisting[language=Python,label=list+prog1,caption={\lstname.  The algorithms.}]{../../src/algorithms.py}

\end{document}

# NOTE file two: first node does not matter, 50% that the sedond gets right -> 0.5OPT, .5^n prop of hitting OPT
